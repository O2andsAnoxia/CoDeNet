{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoDeNet SW\n",
    "Software source code to invoke the CoDeNet accelerator and get the latency for batch size=1 inference time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import cffi\n",
    "import os\n",
    "import numpy as np\n",
    "from pynq import Overlay, Xlnk\n",
    "import pynq\n",
    "import time \n",
    "import math\n",
    "from scipy import signal\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the log level to INFO for now\n",
    "# uncomment this to see per layer details\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoDeNetAccel:\n",
    "    \"\"\"CoDeNet Accelerator Convolution Implementation.\n",
    "\n",
    "    This class keeps all functions to invoke the FPGA accelerator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, MAX_D, MAX_IC, MAX_OC, PA, PE, bitfile, MAX_BATCH=1):\n",
    "        \"\"\"One-off initialization to define the accelerator constraints.\n",
    "\n",
    "        Args:\n",
    "            MAX_D: maximum image width and height\n",
    "            MAX_IC: maximum input channel size\n",
    "            MAX_OC: maximum output channel size\n",
    "            PA: the number of input channels to process in every iteration\n",
    "            PE: the number of output channels to process in every iteration. The total MAC per iteration is PA x PE.\n",
    "            bitfile: the path to the FPGA bitflie\n",
    "        \"\"\"\n",
    "        self.MAX_D = MAX_D\n",
    "        self.MAX_IC = MAX_IC\n",
    "        self.MAX_OC = MAX_OC\n",
    "        self.PA = PA\n",
    "        self.PE = PE\n",
    "        ROOT_DIR = 'bitfile'\n",
    "\n",
    "        # downloads the FPGA bitstream\n",
    "        self.bitstream_name = os.path.join(ROOT_DIR, bitfile)\n",
    "        self.overlay = Overlay(self.bitstream_name)\n",
    "        self.accel = self.overlay.top_0\n",
    "        self.overlay.download()\n",
    "\n",
    "        # resets the accelerator\n",
    "        self.accel.write(0x00, 0x00)\n",
    "        while (self.accel.read(0x00) & 0x1):\n",
    "            time.sleep(0.1)\n",
    "        self.runtime = 0\n",
    "        self.layercount = 0\n",
    "        self.vectoraddtime = 0\n",
    "        self.uptime = 0\n",
    "        self.chansplit = 0\n",
    "        self.convtime = 0\n",
    "        self.collect = []\n",
    "        self.vec_collect = []\n",
    "        self.up_collect = []\n",
    "        self.gops = 0\n",
    "        self.peak_gops = 0\n",
    "        self.shuffletime = 0\n",
    "        self.batch_size = 0\n",
    "\n",
    "        # initializes the size of the buffer\n",
    "        inout_size = MAX_BATCH * MAX_IC * MAX_D * MAX_D // 2  # divided by 2 for 4-bit data\n",
    "        weight_size_3x3dw = 9 * MAX_IC // 2\n",
    "        addr_size = MAX_D * MAX_D\n",
    "        weight_size_1x1 = MAX_IC * MAX_OC // 2\n",
    "        quant_size = MAX_OC\n",
    "\n",
    "        # allocates buffers for input args\n",
    "        self.fmaps = []\n",
    "\n",
    "        # three buffers for feature maps, used alternatively\n",
    "        for _ in range(3):\n",
    "            self.fmaps.append(self.get_pynq_buffer(shape=(inout_size,), dtype=np.uint8, cacheable=1))\n",
    "        self.addr = self.get_pynq_buffer(shape=(addr_size,), dtype=np.uint8)\n",
    "        self.weight_3x3dw = self.get_pynq_buffer(shape=(weight_size_3x3dw,), dtype=np.uint8)\n",
    "        self.weight_1x1 = self.get_pynq_buffer(shape=(weight_size_1x1,), dtype=np.uint8)\n",
    "        self.quant = self.get_pynq_buffer(shape=(quant_size,), dtype=np.int16, cacheable=1)\n",
    "\n",
    "        # get the physical addresses of the buffers\n",
    "        self.fmap_addrs = [fmap.physical_address for fmap in self.fmaps]\n",
    "        self.addr_addr = self.addr.physical_address\n",
    "        self.weight_addr_3x3dw = self.weight_3x3dw.physical_address\n",
    "        self.weight_addr_1x1 = self.weight_1x1.physical_address\n",
    "        self.quant_addr = self.quant.physical_address\n",
    "\n",
    "        # randomly init the weights\n",
    "        self.weight_3x3dw[:] = np.random.randint(16, size=(weight_size_3x3dw,))\n",
    "        self.weight_1x1[:] = np.random.randint(16, size=(weight_size_1x1,))\n",
    "        self.quant[:] = np.ones((quant_size,))\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\" Calculate statistics of the runs.\"\"\"\n",
    "        self.totaltime = self.convtime + self.vectoraddtime + self.uptime + self.chansplit + self.shuffletime\n",
    "\n",
    "    def print_stats(self):\n",
    "        \"\"\" Print statistics of the runs.\"\"\"\n",
    "        print(\"\\n\\tCoDeNet Stats:\")\n",
    "        print(\"\\t\\tconv: %4f s\" % (self.convtime))\n",
    "        print(\"\\t\\tup2: %4f s:\" % (self.uptime))\n",
    "        print(\"\\t\\tchansplit: %4f s\" % (self.chansplit))\n",
    "        print(\"\\t\\tchanshuffle: %4f s\" % (self.shuffletime))\n",
    "        print(\"\\t\\tgops: %4f OP/s\" % (self.gops / self.convtime))\n",
    "        print(\"\\t\\tpeak_gops: %4f OP/s\" % (self.peak_gops))\n",
    "        print(\"\\t\\taccel_latency: %4f s\" % (self.totaltime))\n",
    "        print(\"\\t\\tframerate: %2f fps\" % (1 / self.totaltime))\n",
    "\n",
    "    def get_pynq_buffer(self, shape, dtype, cacheable=0):\n",
    "        \"\"\" Returns the memory buffer that is visible to the FPGA accelertors. \"\"\"\n",
    "        return Xlnk().cma_array(shape, dtype, cacheable=cacheable)\n",
    "\n",
    "    def ceil(self, base, num):\n",
    "        \"\"\"Returns the ceiling of input number given a base factor.\"\"\"\n",
    "        res = num % base\n",
    "        if res != 0:\n",
    "            return num + base - res\n",
    "        else:\n",
    "            return num\n",
    "\n",
    "    def pack_array(self, input, radix: int):\n",
    "        \"\"\"Pack array with radix into uint8 array.\"\"\"\n",
    "        size = len(input)\n",
    "        time = 8 // radix\n",
    "        size = size // time\n",
    "        result = []\n",
    "        for i in range(size):\n",
    "            a = 0\n",
    "            for j in range(time):\n",
    "                a = a | (input[i * time + j] << (radix * j))\n",
    "            result.append(a)\n",
    "        return result\n",
    "\n",
    "    def unpack_array(self, input, radix):\n",
    "        \"\"\"Unpack array with radix into uint8 array.\"\"\"\n",
    "        size = len(input)\n",
    "        time = 8 // radix\n",
    "        mask = (2 ** radix) - 1\n",
    "        result = []\n",
    "        for i in range(size):\n",
    "            a = input[i]\n",
    "            for j in range(time):\n",
    "                b = (a >> (radix * j)) & mask\n",
    "                result.append(b)\n",
    "        return result\n",
    "\n",
    "    def conv(self, x, ic, oc, stride, skip3=0, skip1=0, deform=0, relu1=1, relu3=1):\n",
    "        \"\"\"Deformable convolution kernel.\n",
    "            Args:\n",
    "                x: input tensor in numpy.\n",
    "                ic: input channel size.\n",
    "                oc: output channel size.\n",
    "                stride: stride.\n",
    "                skip3: If true, skip 3x3 dw conv.\n",
    "                skip1: If true, skip 1x1 conv.\n",
    "                deform: If true, run with deformable offsets.\n",
    "                relu1: If true, run relu after 1x1 conv.\n",
    "                relu3: If true, run relu after 3x3 dw conv.\n",
    "        \"\"\"\n",
    "        if oc > 512:\n",
    "            y = self.conv512(x, ic, oc, stride, skip3=skip3, skip1=skip1, deform=deform, relu1=relu1, relu3=relu3)\n",
    "        else:\n",
    "            y = self.base_conv(x, ic, oc, stride, skip3=skip3, skip1=skip1, deform=deform, relu1=relu1, relu3=relu3)\n",
    "        return y\n",
    "\n",
    "    def conv512(self, x, ic, oc, stride, skip3=0, skip1=0, deform=0, relu1=1, relu3=1):\n",
    "        \"\"\"Deformable convolution kernel for oc > 512.\"\"\"\n",
    "        oc_remain = oc\n",
    "        it = 0\n",
    "        ys = []\n",
    "        while (oc_remain > 0):\n",
    "            if (oc_remain - 512) > 0:\n",
    "                oc_size = 512\n",
    "            else:\n",
    "                oc_size = oc_remain\n",
    "\n",
    "            if skip1 == 0:\n",
    "                ic_size = ic\n",
    "                x_in = x\n",
    "            else:\n",
    "                ic_size = oc_size\n",
    "                if (oc_remain - 512) > 0:\n",
    "                    x_in = x[:, :, :, 512 * it: 512 * (it + 1)]\n",
    "                else:\n",
    "                    x_in = x[:, :, :, 512 * it:]\n",
    "            y = self.base_conv(x_in, ic_size, oc_size, stride, skip3=skip3, skip1=skip1, deform=deform, relu1=relu1,\n",
    "                               relu3=relu3)\n",
    "            ys.append(y)\n",
    "\n",
    "            it += 1\n",
    "            oc_remain = oc_remain - 512\n",
    "\n",
    "        start = time.time()\n",
    "        y = np.concatenate(ys, axis=3)\n",
    "        end = time.time()\n",
    "        dur = end - start\n",
    "        self.convtime += dur\n",
    "        return y\n",
    "\n",
    "    def update_args(self, in_addr, out_addr, weight_addr_1x1, weight_addr_3x3dw, quant_addr, addr_addr, fm_d, ic_size,\n",
    "                    oc_size, batch_size, with_stride, skip3, skip1, deform, relu1, relu3):\n",
    "        \"\"\"Update the accelerator args.\n",
    "            These can be hardcoded in memory, we expose them for more flexibility.\n",
    "        \"\"\"\n",
    "        self.accel.write(0x10, in_addr)\n",
    "        self.accel.write(0x18, out_addr)\n",
    "        self.accel.write(0x20, weight_addr_1x1)\n",
    "        self.accel.write(0x28, weight_addr_3x3dw)\n",
    "        self.accel.write(0x30, quant_addr)\n",
    "        self.accel.write(0x38, addr_addr)\n",
    "        self.accel.write(0x40, fm_d)\n",
    "        self.accel.write(0x48, ic_size)\n",
    "        self.accel.write(0x50, oc_size)\n",
    "        self.accel.write(0x58, batch_size)\n",
    "        self.accel.write(0x60, with_stride)\n",
    "        self.accel.write(0x68, skip3)\n",
    "        self.accel.write(0x70, skip1)\n",
    "        self.accel.write(0x78, deform)\n",
    "        self.accel.write(0x80, relu1)\n",
    "        self.accel.write(0x88, relu3)\n",
    "\n",
    "    def base_conv(self, x, ic, oc, stride, skip3=0, skip1=0, deform=0, relu1=1, relu3=1):\n",
    "        \"\"\"Deformable convolution kernel.\n",
    "            Args:\n",
    "                x: input tensor in numpy of shape (batch_size, height, width, input_channel_size).\n",
    "                ic: input channel size.\n",
    "                oc: output channel size.\n",
    "                stride: stride.\n",
    "                skip3: If true, skip 3x3 dw conv.\n",
    "                skip1: If true, skip 1x1 conv.\n",
    "                deform: If true, run with deformable offsets.\n",
    "                relu1: If true, run relu after 1x1 conv. For quantization, with relu, we use symmetric quantization\n",
    "                    with zero point at 0.\n",
    "                relu3: If true, run relu after 3x3 dw conv.\n",
    "        \"\"\"\n",
    "        if stride > 1 and skip3 == 1:\n",
    "            raise ValueError(\"Cannot skip 3x3 dw conv if stride is larger than 1.\")\n",
    "\n",
    "        # batch_size size\n",
    "        batch_size = x.shape[0]\n",
    "        # input feature map dimension\n",
    "        fm_d = x.shape[1]\n",
    "        # input channel size, round to PA's multiples\n",
    "        ic_size = self.ceil(self.PA, ic)\n",
    "        # output channel size, round to PE's multiples\n",
    "        oc_size = self.ceil(self.PE, oc)\n",
    "\n",
    "        if ic_size != x.shape[3]:\n",
    "            raise ValueError(\"x tensor input channel count must be equal to \")\n",
    "\n",
    "        if stride == 2:\n",
    "            y = np.zeros((batch_size, fm_d // 2, fm_d // 2, oc_size), dtype=np.int8)\n",
    "        else:\n",
    "            y = np.zeros((batch_size, fm_d, fm_d, oc_size), dtype=np.int8)\n",
    "\n",
    "        with_stride = 1 if stride == 2 else 0\n",
    "\n",
    "        in_addr = self.fmap_addrs[self.layercount % 2]\n",
    "        out_addr = self.fmap_addrs[(self.layercount + 1) % 2]\n",
    "        weight_addr_1x1 = self.weight_addr_1x1\n",
    "        weight_addr_3x3dw = self.weight_addr_3x3dw\n",
    "        quant_addr = self.quant_addr\n",
    "        addr_addr = self.addr_addr\n",
    "\n",
    "        # passes the args to the accelerator\n",
    "        # this can be stored to memory before invoking the accelerator\n",
    "        self.update_args(in_addr, out_addr, weight_addr_1x1, weight_addr_3x3dw, quant_addr, addr_addr, fm_d,\n",
    "                         ic_size, oc_size, batch_size, with_stride, skip3, skip1, deform, relu1, relu3)\n",
    "\n",
    "        # starts the accel\n",
    "        begin = time.time()\n",
    "        ctrl_val = self.accel.read(0x00)\n",
    "        ready = not (ctrl_val & 0x1)\n",
    "        if not ready:\n",
    "            raise (\"Accelerator not ready!\")        \n",
    "        self.accel.write(0x00, 0x1)\n",
    "\n",
    "        # checks for the done signal\n",
    "        while not (self.accel.read(0x0) & 0x2):\n",
    "            pass\n",
    "\n",
    "        end = time.time()\n",
    "        dur = end - begin\n",
    "\n",
    "        op3 = batch_size * (1 - skip3) * 9 * fm_d * fm_d * ic_size * 2 / 1000000000 / stride / stride\n",
    "        op1 = batch_size * (1 - skip1) * fm_d * fm_d * oc_size * ic_size * 2 / 1000000000 / stride / stride\n",
    "        ops = op1 + op3\n",
    "        self.gops += ops\n",
    "        ops = ops / dur\n",
    "\n",
    "        if ops > self.peak_gops:\n",
    "            self.peak_gops = ops\n",
    "\n",
    "        logging.debug(\"CONV: fm_d=%d ic=%d oc=%d stride=%d skip3=%d skip1=%d deform=%d Time Elapsed=%2f GOPS=%2f.\" % (\n",
    "            fm_d, ic, oc, stride, skip3, skip1, deform, dur, ops))\n",
    "        self.convtime += dur\n",
    "        self.layercount += 1\n",
    "        self.collect.append([fm_d, ic, oc, stride, skip3, dur, ops])\n",
    "\n",
    "        # set y's content to the output content\n",
    "        y[:] = self.fmaps[(self.layercount + 1) % 2][:y.size].reshape(y.shape)\n",
    "        return y\n",
    "\n",
    "    def upsample2(self, x):\n",
    "        \"\"\"Upsamples the feature map by duplicating the pixel. The input size changes from\n",
    "            (d x d x ic) to (2d x 2d x ic).\n",
    "        \"\"\"\n",
    "        fmap_in = x\n",
    "        begin = time.time()\n",
    "        # repeats elements on dim h and w two times\n",
    "        fmap_out = fmap_in.repeat(2, 1).repeat(2, 2)\n",
    "        end = time.time()\n",
    "        dur = end - begin\n",
    "        self.uptime += dur\n",
    "        self.up_collect.append([x.shape[1], x.shape[3], dur])\n",
    "        logging.debug(\"UP2: fm_d=%d ic=%d Time Elapsed=%2f.\" % (x.shape[1], x.shape[3], dur))\n",
    "        return fmap_out\n",
    "\n",
    "    def channel_split(self, x):\n",
    "        \"\"\"Splits the input into two branch from the channel dimension. The input size\n",
    "            changes from (d x d x ic) to (d x d x ic/2).\n",
    "        \"\"\"\n",
    "        begin = time.time()\n",
    "        # take every two element on the 3rd dim\n",
    "        y1 = x[:, :, :, ::2]\n",
    "        # take every two element starting from the 1st element\n",
    "        y2 = x[:, :, :, 1::2]\n",
    "        end = time.time()\n",
    "        dur = end - begin\n",
    "        self.chansplit += dur\n",
    "        logging.debug(\"SPLIT: fm_d=%d ic%d oc=%d Time Elapsed=%2f.\" % (x.shape[1], x.shape[3], y1.shape[3], dur))\n",
    "        return y1, y2\n",
    "\n",
    "    def concat(self, x1, x2):\n",
    "        \"\"\"Concats two branches together from the channel dimension.\"\"\"\n",
    "        begin = time.time()\n",
    "        y = np.concatenate((x1, x2), axis=3)\n",
    "        end = time.time()\n",
    "        dur = end - begin\n",
    "        dur *= self.batch_size\n",
    "        self.vectoraddtime += dur\n",
    "        logging.debug(\"CONCAT: Time Elapsed=%2f.\" % (dur))\n",
    "        return y\n",
    "\n",
    "    def channel_shuffle(self, x, G=2):\n",
    "        \"\"\"Shuffles two branches from the channel dimension.\"\"\"\n",
    "        begin = time.time()\n",
    "        B, H, W, C = x.shape\n",
    "        x = x.reshape(B, H, W, G, C // G)\n",
    "        x = np.transpose(x, (0, 1, 2, 4, 3))\n",
    "        x = x.reshape(B, H, W, C)\n",
    "        end = time.time()\n",
    "        dur = end - begin\n",
    "        self.shuffletime += dur\n",
    "        logging.debug(\"SHUFFLE: Time Elapsed=%2f.\" % (dur))\n",
    "        return x\n",
    "\n",
    "    def concat_and_shuffle(self, x1, x2):\n",
    "        \"\"\"Concats and shuffles two branches at the channel dimension.\"\"\"\n",
    "        begin = time.time()\n",
    "        out = np.zeros((x1.shape[0], x1.shape[1], x1.shape[2], x1.shape[3] + x2.shape[3]), np.uint8)\n",
    "        # every two element's values are from branch 1\n",
    "        out[:, :, :, ::2] = x1\n",
    "        # every two element's values are from branch 2, starting from the 1st element\n",
    "        out[:, :, :, 1::2] = x2\n",
    "        end = time.time()\n",
    "        dur = end - begin\n",
    "        self.shuffletime += dur\n",
    "        logging.debug(\"CONCAT&SHUFFLE: Time Elapsed=%2f.\" % (dur))\n",
    "        return out\n",
    "\n",
    "    def BaseNode(self, x, inp, oup, stride):\n",
    "        \"\"\"Runs basic building blocks.\n",
    "            if stride is 1, then runs:\n",
    "                concat_and_shuffle(1x1+3x3dw+1x1(split(in)), split(in))\n",
    "            else runs:\n",
    "                concat_and_shuffle(3x3dw+1x1(in), (1x1+3x3dw+1x1(in)))\n",
    "        \"\"\"\n",
    "        oup_inc = oup // 2\n",
    "        if stride == 1:\n",
    "            y1, y2 = self.channel_split(x)\n",
    "            y1 = self.conv(y1, oup_inc, oup_inc, stride=1, skip1=0, skip3=0, relu1=1, relu3=0, deform=0)\n",
    "            y2 = self.conv(y2, oup_inc, oup_inc, stride=1, skip3=1, relu1=1, deform=0)\n",
    "            y = self.concat_and_shuffle(y1, y2)\n",
    "        elif stride == 2:\n",
    "            # bn1:\n",
    "            y = self.conv(x, inp, inp, skip1=1, stride=2, relu3=0, deform=0)\n",
    "            y1 = self.conv(y, inp, oup_inc, skip3=1, stride=1, relu1=1, deform=0)\n",
    "            # bn2:\n",
    "            y = self.conv(x, inp, oup_inc, stride=2, relu3=0, relu1=1, deform=0)\n",
    "            y2 = self.conv(y, oup_inc, oup_inc, stride=1, skip3=1, relu1=1, deform=0)\n",
    "            y = self.concat_and_shuffle(y1, y2)\n",
    "        return y\n",
    "\n",
    "    def HeadConv(self, x, ic, oc):\n",
    "        \"\"\"Runs to generate the labels for class, center, or box.\"\"\"\n",
    "        x = self.conv(x, ic, oc, stride=1, relu1=1, relu3=0, deform=0)\n",
    "        return x\n",
    "\n",
    "    def deform_conv(self, x, ic, oc):\n",
    "        \"\"\"Runs deformable convolution.\"\"\"\n",
    "        # first generates the offset with the 1x1 conv\n",
    "        scale = self.conv(x, ic, 1, stride=1, skip3=1, skip1=0, relu1=0, deform=0)\n",
    "\n",
    "        # then runs a 3x3 dw conv with the offset\n",
    "        x = self.conv(x, ic, ic, stride=1, skip3=0, skip1=1, relu3=0, deform=1)\n",
    "\n",
    "        if ic != oc:\n",
    "            # runs a 1x1 conv to change the channel size if needed\n",
    "            conv_channel = self.conv(x, ic, oc, stride=1, skip3=1, relu1=0, deform=0)\n",
    "            return conv_channel\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def CoDeNet(self, x, w2=False):\n",
    "        \"\"\"\"\"\"\n",
    "        if w2:\n",
    "            self.channels = [32, 256, 512, 1024, 2048]\n",
    "            deconv_planes = [2048, 256, 128]\n",
    "        else:\n",
    "            self.channels = [32, 128, 256, 512, 1024]\n",
    "            deconv_planes = [1024, 256, 128]\n",
    "        stage_repeats = [3, 7, 3]\n",
    "        num_filters = [256, 128, 128]\n",
    "\n",
    "        heads = [80, 2, 2]\n",
    "\n",
    "        for idx in range(len(stage_repeats)):\n",
    "            x = self.BaseNode(x, self.channels[idx], self.channels[idx + 1], 2)\n",
    "            for _ in range(stage_repeats[idx]):\n",
    "                x = self.BaseNode(x, self.channels[idx], self.channels[idx + 1], 1)\n",
    "        x = self.conv(x, self.channels[3], self.channels[4], stride=1, skip3=1, skip1=0, deform=0, relu1=1)\n",
    "\n",
    "        for i in range(len(num_filters)):\n",
    "            x = self.deform_conv(x, deconv_planes[i], num_filters[i])\n",
    "            x = self.upsample2(x)\n",
    "\n",
    "        ret = []\n",
    "        for head in heads:\n",
    "            out = self.HeadConv(x, 128, head)\n",
    "            ret.append(ret)\n",
    "        self.get_stats()\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Example Code\n",
    "Code to invoke the accelerator with input size of batch size=1, height=64, width=64, input channel size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tCoDeNet Stats:\n",
      "\t\tconv: 0.019765 s\n",
      "\t\tup2: 0.000563 s:\n",
      "\t\tchansplit: 0.000125 s\n",
      "\t\tchanshuffle: 0.004843 s\n",
      "\t\tgops: 28.991496 OP/s\n",
      "\t\tpeak_gops: 77.498617 OP/s\n",
      "\t\taccel_latency: 0.025296 s\n",
      "\t\tframerate: 39.532357 fps\n"
     ]
    }
   ],
   "source": [
    "accel = CoDeNetAccel(MAX_D=128, MAX_IC=1024, MAX_OC=1024, PA=16, PE=16, bitfile=\"batch.bit\")\n",
    "x = np.random.randint(256, size=(1,64,64,32))\n",
    "y = accel.CoDeNet(x, w2=False)\n",
    "accel.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experiment Configurations and Results\n",
    "This part runs the CoDeNet on the accelerator and reports the latency of running the accelerator and running 1st layer and other auxiliary functions on CPU. \n",
    "We have verified the deformable conv kernel correctness in Vivado HLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpConfig:\n",
    "    \"\"\"CoDeNet Accelerator Configurations.\"\"\"\n",
    "\n",
    "    def __init__(self, name, in_size, w2, first_latency):\n",
    "        \"\"\"\n",
    "        Initializes config. \n",
    "        The first layer is run on CPU due to 8-bit weights.\n",
    "        We obtain the first layer latency using TVM (see ./sw/tvm/README.md). \n",
    "            Args:\n",
    "                name: config name corresponding to table 3&5 in the paper.\n",
    "                in_size: input size to the accelerator pipeline.\n",
    "                w2: whether to double the channel size in the network to improve accuracy.\n",
    "                first_latency: is the first layer latency we obtain from the TVM runs. \n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.in_size = in_size\n",
    "        self.w2 = w2\n",
    "        self.first_latency = first_latency \n",
    "        self.transpose_input()\n",
    "        \n",
    "    def transpose_input(self):\n",
    "        \"\"\"Time the transpose overhead of each image from nchw to nhwc layout. \"\"\"\n",
    "        orig_layout = (self.in_size[0], self.in_size[3], self.in_size[1], self.in_size[2])\n",
    "        x = np.random.randint(256, size=orig_layout)\n",
    "        start = time.time()\n",
    "        y = x.transpose((0, 2, 3, 1))\n",
    "        end = time.time()\n",
    "        dur = end - start \n",
    "        self.first_latency += dur * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Experiements: \n",
      "\n",
      "\tCoDeNet Stats:\n",
      "\t\tconv: 0.019726 s\n",
      "\t\tup2: 0.000563 s:\n",
      "\t\tchansplit: 0.000123 s\n",
      "\t\tchanshuffle: 0.004858 s\n",
      "\t\tgops: 29.048963 OP/s\n",
      "\t\tpeak_gops: 78.014129 OP/s\n",
      "\t\taccel_latency: 0.025271 s\n",
      "\t\tframerate: 39.571519 fps\n",
      "\tconfig a takes 27.114093 ms to finish.\n",
      "\n",
      "\tCoDeNet Stats:\n",
      "\t\tconv: 0.019799 s\n",
      "\t\tup2: 0.000521 s:\n",
      "\t\tchansplit: 0.000124 s\n",
      "\t\tchanshuffle: 0.004867 s\n",
      "\t\tgops: 28.941224 OP/s\n",
      "\t\tpeak_gops: 77.712583 OP/s\n",
      "\t\taccel_latency: 0.025311 s\n",
      "\t\tframerate: 39.508153 fps\n",
      "\tconfig b takes 33.241988 ms to finish.\n",
      "\n",
      "\tCoDeNet Stats:\n",
      "\t\tconv: 0.065977 s\n",
      "\t\tup2: 0.003505 s:\n",
      "\t\tchansplit: 0.000142 s\n",
      "\t\tchanshuffle: 0.020069 s\n",
      "\t\tgops: 34.740178 OP/s\n",
      "\t\tpeak_gops: 94.772719 OP/s\n",
      "\t\taccel_latency: 0.089693 s\n",
      "\t\tframerate: 11.149163 fps\n",
      "\tconfig c takes 94.741707 ms to finish.\n",
      "\n",
      "\tCoDeNet Stats:\n",
      "\t\tconv: 0.124250 s\n",
      "\t\tup2: 0.002711 s:\n",
      "\t\tchansplit: 0.000179 s\n",
      "\t\tchanshuffle: 0.040452 s\n",
      "\t\tgops: 53.451984 OP/s\n",
      "\t\tpeak_gops: 100.887088 OP/s\n",
      "\t\taccel_latency: 0.167592 s\n",
      "\t\tframerate: 5.966862 fps\n",
      "\tconfig d takes 172.636633 ms to finish.\n",
      "\n",
      "\tCoDeNet Stats:\n",
      "\t\tconv: 0.124005 s\n",
      "\t\tup2: 0.002501 s:\n",
      "\t\tchansplit: 0.000194 s\n",
      "\t\tchanshuffle: 0.040149 s\n",
      "\t\tgops: 53.557528 OP/s\n",
      "\t\tpeak_gops: 100.968515 OP/s\n",
      "\t\taccel_latency: 0.166849 s\n",
      "\t\tframerate: 5.993447 fps\n",
      "\tconfig e takes 197.292529 ms to finish.\n"
     ]
    }
   ],
   "source": [
    "# Since we quantize the first layer to 8-bit weights, we need to run it on the ARM CPU. \n",
    "# Please see ./sw/tvm/README.md on how to run the first conv+pooling layer on TVM. \n",
    "configs = [\n",
    "    ExpConfig(name='config a', in_size=(1, 64, 64, 32), w2=False, first_latency=1.8),\n",
    "    ExpConfig(name='config b', in_size=(1, 64, 64, 32), w2=False, first_latency=7.9),\n",
    "    ExpConfig(name='config c', in_size=(1, 128, 128, 32), w2=False, first_latency=5.0),\n",
    "    ExpConfig(name='config d', in_size=(1, 128, 128, 32), w2=True, first_latency=5.0),\n",
    "    ExpConfig(name='config e', in_size=(1, 128, 128, 32), w2=True, first_latency=30.4),\n",
    "]\n",
    "\n",
    "print(\"Run Experiements: \")\n",
    "for config in configs: \n",
    "    accel = CoDeNetAccel(MAX_D=128, MAX_IC=1024, MAX_OC=1024, PA=16, PE=16, bitfile=\"batch.bit\")\n",
    "    # init input with random int\n",
    "    x = np.random.randint(256, size=config.in_size)\n",
    "    y = accel.CoDeNet(x, w2=config.w2)\n",
    "    accel.print_stats()\n",
    "    dur = config.first_latency + accel.totaltime * 1000 \n",
    "    print(\"\\t%s takes %1f ms to finish.\"%(config.name, dur))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
